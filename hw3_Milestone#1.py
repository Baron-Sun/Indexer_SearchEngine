import os
from lxml import etree
from bs4 import BeautifulSoup
from io import StringIO
import re,string
import json
import pickle
from nltk.stem import PorterStemmer

def tokenize(content):
    token = re.findall(r"[A-Za-z0-9]+", content.lower())
    return token

# Take 3 parameters: fileLocation, urlDocIdMap, invertedIndex
# fileLocation is the file location to extract data. We assume the file is in json
# urlDocIdMap is a global map to save the mapping data of url. (Key: url, Value: uniqueDocumentId(generated by us))
# docIdUrlMap is also a global map used to get correct url during searching. (Key: uniqueDocumentId(generated by us), Value: url)
# invertedIndex is the global inverted index to save the data of each document, the format is:
    # key: token; value: {totalFreq: INT, 'docMap': { key: docId:INT; value: {rank: INT, positions:[INT]} } }
# Main Task: load the json data in "fileLocation" into invertedIndex, urlDocIdMap, and docIdUrlMap
def indexingFile(fileLocation, urlDocIdMap, docIdUrlMap, invertedIndex):
    try:
        print('file location is: ', fileLocation)
        fileObj = open(fileLocation)
        wholeFile = json.loads(fileObj.read())
        url, content = wholeFile['url'], wholeFile['content'] # get url and content from the file
        fileObj.close()

        # Implement the urlDocIdMap and create docId
        if url not in urlDocIdMap.keys():
            uniqueDocId = len(urlDocIdMap)
            urlDocIdMap[url] = uniqueDocId
            docIdUrlMap[uniqueDocId] = url
        else:
            uniqueDocId = urlDocIdMap[url]

        # Initialize rankDictionary with algorithm: totalRank of one token in one document = numTitle * 3 + numHead * 2 + b/strongNum * 1
        tagRankingDict = {'title': 10, 'h1': 8, 'h2': 6, 'h3': 4, 'b': 3, 'strong': 3, 'p': 1, 'span': 1, "li": 1, "a": 1, "cite": 1, "em": 1, "mark": 1, "b": 1, "abbr": 1}

        # Create a word dictionary to save all
        soup = BeautifulSoup(content, 'lxml')
        text = soup.find_all(text=True)
        pos = 0
        ps = PorterStemmer()
        for t in text:
            if t.parent.name in tagRankingDict.keys():
                rank = tagRankingDict[t.parent.name]
                for token in tokenize(t):
                    token = ps.stem(token) # using porter stemming
                    if(len(token) < 3 or len(token) > 10):
                        continue
                    # if token not in invertedIndex, create an object
                    if token not in invertedIndex.keys():
                        invertedIndex[token] = {'totalFreq': 1, 'docDict': {uniqueDocId: {'rank': rank, 'positions': [pos]} } }
                    # if token was in invertedIndex, update it
                    else:
                        invertedIndex[token]['totalFreq'] += 1
                        # if document hasn't been stored in the same token, store it
                        if uniqueDocId not in invertedIndex[token]['docDict'].keys():
                            invertedIndex[token]['docDict'][uniqueDocId] = {'rank': rank, 'positions': [pos]}
                        # if document has been stored in the same token, update it
                        else:
                            invertedIndex[token]['docDict'][uniqueDocId]['rank'] += rank
                            invertedIndex[token]['docDict'][uniqueDocId]['positions'].append(pos)
                    pos += 1
    except ValueError as e:
        print("Error in File Location:", fileLocation)
        print(e)

# 1 paramter: folderLocation
# Main Task: explore all file in "folderLocation" with method indexingFile
def indexingInvertedTable(folderLocation, urlDocIdMapAddress, docIdUrlMapAddress):
    # Processing folder DEV
    invertedIndex = {} # key: token; value: {totalFreq: INT, docMap: { Key: docId:INT; value: {rank: INT, positions:[INT]} } }
    urlDocIdMap = {} # 
    with open(urlDocIdMapAddress, 'rb') as f:
        urlDocIdMap = pickle.load(f) # get urlDocIdMap; key: url of website, value: unique docId of each url
    with open(docIdUrlMapAddress, 'rb') as f: 
        docIdUrlMap = pickle.load(f) # get docIdUrlMap; key: unique docId of each url, value: url of website

    for (root, dirs, files) in os.walk('DEV/' + folderLocation):
        for filename in files:
            fileLocation = os.path.join(root, filename)
            try:
                # print('file_location is:', fileLocation)
                indexingFile(fileLocation, urlDocIdMap, docIdUrlMap, invertedIndex)
            except ValueError as e:
                print("File Location:", fileLocation)
                print(e)

    # save the Inverted Table into disk            
    with open('./map_result/' + folderLocation , 'wb') as f:
        pickle.dump(invertedIndex, f)

    # save the url Map into disk     
    with open(urlDocIdMapAddress, 'wb') as f:
        pickle.dump(urlDocIdMap, f)
    with open(docIdUrlMapAddress, 'wb') as f:
        pickle.dump(docIdUrlMap, f)

if __name__ == '__main__':
    print("hello world")
    folderLocation1 = 'loading1'
    folderLocation2 = 'loading2'
    folderLocation3 = 'loading3'
    urlDocIdMapAddress = './maps/urlDocIdMap'
    docIdUrlMapAddress = './maps/docIdUrlMap'
    indexingInvertedTable(folderLocation3, urlDocIdMapAddress, docIdUrlMapAddress)
    
